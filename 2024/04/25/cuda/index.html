<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="从零开始的CUDA编程, ACM,C++,algorithm">
    <meta name="description" content="GPU简介近年来，随着深度学习的快速发展，GPU在计算领域扮演着越来越重要的角色，已经成为了人工智能和机器学习领域的关键组件。进入大模型时代，模型近乎恐怖的算力需求需要高性能的计算资源来完成，而 GPU 具有大量的计算核心和高速的显存，使得">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>从零开始的CUDA编程 | Knowledge</title>
    <link rel="icon" type="image/png" href="/my_favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.1.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/my_logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Knowledge</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/my_logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Knowledge</div>
        <div class="logo-desc">
            
            share life
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/knowledge-llz" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/knowledge-llz" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/22.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">从零开始的CUDA编程</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/cuda%E7%BC%96%E7%A8%8B/">
                                <span class="chip bg-color">cuda编程</span>
                            </a>
                        
                            <a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">
                                <span class="chip bg-color">并行计算</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                高性能计算
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-04-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-06-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="GPU简介"><a href="#GPU简介" class="headerlink" title="GPU简介"></a>GPU简介</h2><p>近年来，随着深度学习的快速发展，GPU在计算领域扮演着越来越重要的角色，已经成为了人工智能和机器学习领域的关键组件。进入大模型时代，模型近乎恐怖的算力需求需要高性能的计算资源来完成，而 GPU 具有大量的计算核心和高速的显存，使得它能够高效地处理这些计算任务，从而成为了大模型训练和推理不可缺少的工具。</p>
<p>插播一条新闻：</p>
<blockquote>
<p>Meta engineers trained Llama 3 on computer clusters packing 24,576 <a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a>, linked with RoCE and <a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/networking/quantum2/">NVIDIA Quantum-2 InfiniBand</a> networks.</p>
<p>To further advance the state of the art in <a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>, Meta <a target="_blank" rel="noopener" href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">recently described</a> plans to scale its infrastructure to 350,000 H100 GPUs.</p>
<p>–From <a target="_blank" rel="noopener" href="https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/">Wide Open: NVIDIA Accelerates Inference on Meta Llama 3 | NVIDIA Blog</a></p>
</blockquote>
<p><strong>为什么深度学习需要使用GPU？</strong></p>
<p>这个答案可以从下面这张图中看出端倪：</p>
<ol>
<li>绿色部分是计算单元，GPU有着更多的计算核心，计算能力更强。</li>
<li>黄色部分是控制单元，CPU中有大量的控制单元，现代CPU的晶体管越来越复杂，除了“计算”，还要实现乱序执行、分支预测、高速缓存等功能；而GPU中控制单元很少，是专门用于并行计算</li>
</ol>
<p>深度学习中存在大量矩阵运算都是并行执行的，GPU非常适合对这类高度并行性的任务进行计算加速</p>
<p><img src="/2024/04/25/cuda/cpu_gpu.png" alt="CPU VS GPU"></p>
<h3 id="GPU架构"><a href="#GPU架构" class="headerlink" title="GPU架构"></a>GPU架构</h3><p>Nvdia每几年都会提出新的GPU架构，并且都是以著名的物理学家对架构进行命名。但这些更新主要是增加硬件单元的数量，组成结构基本没有变化。</p>
<p><img src="/2024/04/25/cuda/image-20240425205927057.png" alt="GPU ROADMAP"></p>
<p>下图展示了GPU的Fermi 架构（其他架构大体结构基本相同，主要是硬件数量的变化）。</p>
<p>一块GPU上存在多个Streaming Multiprocessor（SM），每个SM上有很多个计算核心（Core）、线程调度器（Warp Scheduler）、Shared Memory（共享内存）、SFU（特殊运算单元，如sin、cos）、Register File（寄存器组）。除了每个SM上的Share Memory，所有SM会共享Global Memory。</p>
<p>可以看到GPU这样的硬件结构专为大规模并行和高吞吐量而设计的，下面我们结合这种硬件结构具体讨论GPU是如何进行并行计算。</p>
<p>Flynn将计算设备分为四类：</p>
<ul>
<li>SISD：单条指令操作一条数据，例如之前介绍的简单流水线</li>
<li>MISD：多条指令操作一条数据，很少</li>
<li>MIMD：多条指令操作多条数据，例如VLIW</li>
<li>SIMD：单条指令操作多条数据， 例如Vector Processor，GPU</li>
</ul>
<p>GPU则是SIMD中的一种，也就是说它一条指令可以操作多条数据，更具体地说，GPU采用的是一种SIMT（Single-Instruction, Multiple-Thread）的架构，也就是一条指令可以执行多个线程。在程序运行时，GPU会同时执行大量相同指令的线程。而这些线程首先会通过一个调度器，调度到闲置的SM上，再由SM内部的Warp Scheduler调度到Core上进行计算。目前，我们还没有介绍CUDA中的概念，下一节我们将结合CUDA编程模型中的概念再一次讲述整个调度过程。</p>
<p><img src="/2024/04/25/cuda/gpu_arch.jpg" alt="GPU架构"></p>
<p><img src="/2024/04/25/cuda/gpu_sm.jpg" alt="GPU存储结构"></p>
<p><img src="/2024/04/25/cuda/image-20240425224603536.png" alt="G80 CUDA Mode"></p>
<h2 id="CUDA简介"><a href="#CUDA简介" class="headerlink" title="CUDA简介"></a>CUDA简介</h2><p>GeForce 256是英伟达1999年开发的第一个GPU，最初只用作显示器上渲染高端图形，只用于像素计算。在早期，OpenGL和DirectX等图形API是与GPU唯一的交互方式。后来，人们意识到GPU除了用于渲染图形图像外，还可以做其他的数学计算，而OpenGL和DirectX等图形API的交互方式比较复杂，不利于程序员设计GPU计算，这促成了CUDA编程框架的开发，它提供了一种与GPU交互的简单而高效的方式。</p>
<h3 id="CUDA环境准备"><a href="#CUDA环境准备" class="headerlink" title="CUDA环境准备"></a>CUDA环境准备</h3><p>在开启cuda编程之前，首先需要检查开发环境是否具备必要的条件：</p>
<ul>
<li>Nvidia的GPU</li>
<li>Nvidia的显卡驱动</li>
<li>标准的C编译器</li>
<li>CUDA开发工具</li>
</ul>
<p>由于我直接用了建立好的环境，建立环境的过程在此博客就不展开，可以参考以下网址：</p>
<h5 id="CUDA-编程指南"><a href="#CUDA-编程指南" class="headerlink" title="CUDA 编程指南"></a>CUDA 编程指南</h5><p><a target="_blank" rel="noopener" href="https://www.nvidia.cn/docs/IO/51635/NVIDIA_CUDA_Programming_Guide_1.1_chs.pdf">CUDA编程指南</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">CUDA 官方文档</a></p>
<h5 id="CUDA-开发环境"><a href="#CUDA-开发环境" class="headerlink" title="CUDA 开发环境"></a>CUDA 开发环境</h5><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/580156606">如何在Docker中搭建CUDA &amp; CUDNN 开发环境</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/631850036">为各种 NVIDIA 架构匹配 CUDA arch 和 CUDA gencode</a></p>
<p>建立好CUDA开发环境之后，可以通过以下命令进行检查：</p>
<pre class="line-numbers language-sh" data-language="sh"><code class="language-sh">nvidia-smi<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>应该看到：</p>
<p><img src="/2024/04/25/cuda/image-20240426155445124.png" alt="nvidia-smi执行"></p>
<pre class="line-numbers language-sh" data-language="sh"><code class="language-sh">nvcc --version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><img src="/2024/04/25/cuda/image-20240426155543220.png" alt="检查 nvcc"></p>
<h3 id="CUDA编程模型"><a href="#CUDA编程模型" class="headerlink" title="CUDA编程模型"></a>CUDA编程模型</h3><p>在开始CUDA编程之前，首先要了解CUDA中涉及到的概念：“grid”、“block”、“thread”。</p>
<ul>
<li><p>thread：一个CUDA的并行程序会被以许多个thread来执行</p>
</li>
<li><p>block: 多个线程组成一个线程块（Block），同一个block的线程会被调度到同一个SM上，即同一个block的thread可以进行同步并可用SM上的share memory通信，不同block的thread无法通信。</p>
</li>
<li><p>grid: CUDA的一个函数叫做一个kernel，一个kernel会发起大量执行相同指令的线程</p>
</li>
</ul>
<p><img src="/2024/04/25/cuda/image-20240426152701273.png" alt="cuda编程软件层次"></p>
<p>这三个概念是CUDA编程中最核心的，只要知道这些，我们就已经可以写cuda代码了，那些SM、Share Memory等硬件概念不知道都没有关系，但了解硬件结构可以帮助我们更好地对cuda代码深度优化。</p>
<p>既然知道“grid”、“block”、“thread”这些概念就可以写cuda程序了，那我们就尝试编写一个cuda程序<code>hello-gpu.cu</code>，让GPU输出“Hello World！”。</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>

<span class="token keyword">void</span> <span class="token function">helloCPU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello World!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>


__global__ <span class="token keyword">void</span> <span class="token function">helloGPU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello World! --From GPU\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>


<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>

  <span class="token function">helloCPU</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  helloGPU<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到cuda程序和普通的c语言非常相似，下面我们讲讲不一样的地方：</p>
<ul>
<li><code>__global__</code>：定义这是一个cuda的kernel函数，从主机<code>host</code>发起并在设备<code>device</code>上执行。</li>
<li><code>&lt;&lt;&lt;1, 1&gt;&gt;&gt;</code>：定义block和threads，这里表示发起1个block，每个block里有1个线程</li>
<li><code>cudaDeviceSynchronize</code>：与许多 C&#x2F;C++ 代码不同，核函数启动方式为<strong>异步</strong>：CPU 代码将继续执行而无需等待核函数完成启动。调用 CUDA 运行时提供的函数 <code>cudaDeviceSynchronize</code> 将导致主机 (CPU) 代码暂作等待，直至设备 (GPU) 代码执行完成，才能在 CPU 上恢复执行。</li>
</ul>
<p>写好cuda代码后，我们可以使用<code>nvcc</code>对代码进行编译与执行：</p>
<pre class="line-numbers language-sh" data-language="sh"><code class="language-sh">nvcc -arch&#x3D;sm_61 -o hello-gpu hello-gpu.cu -run<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ul>
<li><code>nvcc</code> 是使用 <code>nvcc</code> 编译器的命令行命令。</li>
<li>将 <code>xxx.cu</code> 作为文件传递以进行编译。</li>
<li><code>o</code> 标志用于指定编译程序的输出文件。</li>
<li><code>arch</code> 标志表示该文件必须编译为哪个<strong>架构</strong>类型。本示例中，<code>sm_61</code> 将用于专门针对本实验运行的 NVIDIA GeForce GTX 1080 Ti 进行编译，但有意深究的用户可以参阅有关 <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-steering-gpu-code-generation"><code>arch</code> 标志</a>、<a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list">虚拟架构特性</a> 和 <a target="_blank" rel="noopener" href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list">GPU特性</a> 的文档。</li>
<li>为方便起见，提供 <code>run</code> 标志将执行已成功编译的二进制文件。</li>
</ul>
<p>从上面的程序，我们可以知道GPU的工作任务是由CPU触发的，GPU自身是无法独立工作的。cuda程序整体的工作流程是CPU将需要执行的任务异步地交给GPU，再由GPU进行调度，最后再将计算结果同步给CPU。</p>
<p><img src="/2024/04/25/cuda/gpu_code.png" alt="CPU和GPU协同工作"></p>
<p>假设我们想要GPU发送66个”Hello World”，我们可以简单地修改blocks和ThreadsPerBlock的数量，即可实现这项功能：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>

<span class="token keyword">void</span> <span class="token function">helloCPU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello World!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>


__global__ <span class="token keyword">void</span> <span class="token function">helloGPU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello World! --From GPU\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>


<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>

  <span class="token function">helloCPU</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  helloGPU<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>以上代码则发起了6个block，每个block里有11个线程。当然，我们也可以改成<code>helloGPU&lt;&lt;&lt;1, 66&gt;&gt;&gt;();</code>，发起了一个block，这个block里有66个线程。</p>
<h3 id="Warp"><a href="#Warp" class="headerlink" title="Warp"></a>Warp</h3><p>具体怎么设置发起blocks和ThreadsPerBlock完全由程序员自己设置，而发起后这些block和线程在GPU中如何调度则由GPU内部硬件控制，不被程序员所操作。但为了更合理地设置blocks和ThreadsPerBlock，我们还是需要了解GPU中的调度策略。</p>
<p>首先是blocks的调度，同一个blocks会被调度到同一个SM，不同的blocks不保证在同一SM，为了更好地进行调度，blocks数可以设置为GPU中SM的整数倍。由于SM上的计算单元是有限的，同一个blocks中的threads会被划分成多个warp，一个warp才是GPU调度与执行的基本单元。一般来说，一个warp是32个线程，所以ThreadsPerBlock一般会设置成32的整数倍，可以让资源利用率更高。</p>
<p>了解了GPU中的调度逻辑，编写cuda程序时我们就可以根据手中的GPU硬件配置，合理地设置blocks和ThreadsPerBlock这两个参数。当前GPU硬件配置有很多内容，这会让我们目不暇接，在初学CUDA编程中应该关注到的是GPU上SM数量，warp size，每个block的最大线程数，每个SM最大block数。下面我们通过这段代码将GPU硬件信息打印出来。</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> dev <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    cudaDeviceProp devProp<span class="token punctuation">;</span>
    <span class="token function">cudaGetDeviceProperties</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>devProp<span class="token punctuation">,</span> dev<span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"使用GPU device "</span> <span class="token operator">&lt;&lt;</span> dev <span class="token operator">&lt;&lt;</span> <span class="token string">": "</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>name <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"SM的数量："</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>multiProcessorCount <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    <span class="token keyword">int</span> warpSize <span class="token operator">=</span> devProp<span class="token punctuation">.</span>warpSize<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Warp size: "</span> <span class="token operator">&lt;&lt;</span> warpSize <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"每个线程块的共享内存大小："</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>sharedMemPerBlock <span class="token operator">/</span> <span class="token number">1024.0</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" KB"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"每个线程块的最大线程数："</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>maxThreadsPerBlock <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"每个SM的最大线程数："</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>maxThreadsPerMultiProcessor <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"每个SM的最大block数："</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>maxThreadsPerMultiProcessor <span class="token operator">/</span> warpSize <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"每个SM的寄存器数量："</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>regsPerMultiprocessor <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-sh" data-language="sh"><code class="language-sh">nvcc -o gpu_check gpu_check.cu -run<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>执行指令得到以下信息：</p>
<blockquote>
<p>使用GPU device 0: NVIDIA GeForce GTX 1080 Ti<br>SM的数量：28<br>Warp size: 32<br>每个线程块的共享内存大小：48 KB<br>每个线程块的最大线程数：1024<br>每个SM的最大线程数：2048<br>每个SM的最大block数：64<br>每个SM的寄存器数量：65536</p>
</blockquote>
<p>下面举一个简单的例子来说明如果根据硬件配置合理分配资源，假设一个SM上有8192个寄存器，程序员每个block设置了256个线程。</p>
<p>假设每个线程会占用10个寄存器，那么一个block中的线程会占用256*10&#x3D;2560个寄存器，8192&#x2F;2560&#x3D;3.2，即一个SM可以同时加载3个block正常运行。</p>
<p>假设每个线程会占用11个寄存器，那么一个block中的线程会占用256*11&#x3D;2816个寄存器，8192&#x2F;2816&#x3D;2.9，即一个SM只能加载2个block，一个SM上硬件资源就跑不满，会造成资源浪费。</p>
<img src="image-20240427152716007.png" alt="Blocks调度到SM" style="zoom: 67%;" />

<p><img src="/2024/04/25/cuda/image-20240427152803303.png" alt="切分成warp"></p>
<p>由于GPU没有复杂的控制单元，在warp中所有线程都会执行相同的指令，这意味着在遇到分支时，warp需要一些特殊的处理。如下图所示，当遇到分支时，warp中32个线程也许有些线程满足条件，有些线程不满足条件，但一个warp中所有线程执行指令的时序是一致的，不满足分支条件的线程必须等待需要执行指令的其他线程，这也意味着分支指令会影响GPU的运行效率，在程序设计时应该尽量少用，或者在写分支条件时尽可能保证一个warp中所有线程同时满足条件或者同时不满足条件。</p>
<p><img src="/2024/04/25/cuda/warp-branch.png" alt="warp分支处理"></p>
<p>最后提一嘴，warp为什么叫warp？warp的英文意思有”编织物的纱线”，我们看下面这张图，假如说一个thread就是一条线，warp则是在机器上并发处理的很多条线。</p>
<p><img src="/2024/04/25/cuda/image-20240427152959024.png" alt="warp"></p>
<h2 id="CUDA编程实战"><a href="#CUDA编程实战" class="headerlink" title="CUDA编程实战"></a>CUDA编程实战</h2><h3 id="Which-Thread"><a href="#Which-Thread" class="headerlink" title="Which Thread?"></a>Which Thread?</h3><p>CUDA程序中提供了 <code>blockIdx</code>, <code>threadIdx</code>, <code>blockDim</code>, <code>GridDim</code>来定位发起thread，下面我们发起1个grid，里面有2个block，每个block里有5个threads。</p>
<p>程序让每个thread输出自己的id号。</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span><span class="token string">&lt;stdio.h></span></span>

__global__ <span class="token keyword">void</span> <span class="token function">print_id</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> id <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"This is thread %d\n"</span><span class="token punctuation">,</span> id<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    print_id<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Vector-ADD"><a href="#Vector-ADD" class="headerlink" title="Vector ADD"></a>Vector ADD</h3><p>下面我们将向量a与向量b逐元素相加，计算的结果为向量c。使用cpu与GPU分别实现向量相加，并比较执行速度。</p>
<p>这里用到了”cudastart.h”定义了计算时间和初始化的函数：</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">ifndef</span> <span class="token expression">CUDASTART_H</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">CUDASTART_H</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">CHECK</span><span class="token expression"><span class="token punctuation">(</span>call<span class="token punctuation">)</span></span><span class="token punctuation">\</span>
<span class="token expression"><span class="token punctuation">&#123;</span></span><span class="token punctuation">\</span>
  <span class="token expression"><span class="token keyword">const</span> <span class="token class-name">cudaError_t</span> error<span class="token operator">=</span>call<span class="token punctuation">;</span></span><span class="token punctuation">\</span>
  <span class="token expression"><span class="token keyword">if</span><span class="token punctuation">(</span>error<span class="token operator">!=</span>cudaSuccess<span class="token punctuation">)</span></span><span class="token punctuation">\</span>
  <span class="token expression"><span class="token punctuation">&#123;</span></span><span class="token punctuation">\</span>
      <span class="token expression"><span class="token function">printf</span><span class="token punctuation">(</span></span><span class="token string">"ERROR: %s:%d,"</span><span class="token expression"><span class="token punctuation">,</span><span class="token constant">__FILE__</span><span class="token punctuation">,</span><span class="token constant">__LINE__</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token punctuation">\</span>
      <span class="token expression"><span class="token function">printf</span><span class="token punctuation">(</span></span><span class="token string">"code:%d,reason:%s\n"</span><span class="token expression"><span class="token punctuation">,</span>error<span class="token punctuation">,</span><span class="token function">cudaGetErrorString</span><span class="token punctuation">(</span>error<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token punctuation">\</span>
      <span class="token expression"><span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span><span class="token punctuation">\</span>
  <span class="token expression"><span class="token punctuation">&#125;</span></span><span class="token punctuation">\</span>
<span class="token expression"><span class="token punctuation">&#125;</span></span></span>


<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;time.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">ifdef</span> <span class="token expression">_WIN32</span></span>
<span class="token macro property"><span class="token directive-hash">#</span>	<span class="token directive keyword">include</span> <span class="token string">&lt;windows.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">else</span></span>
<span class="token macro property"><span class="token directive-hash">#</span>	<span class="token directive keyword">include</span> <span class="token string">&lt;sys/time.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span></span>

<span class="token keyword">double</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">struct</span> <span class="token class-name">timeval</span> tp<span class="token punctuation">;</span>
  <span class="token function">gettimeofday</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>tp<span class="token punctuation">,</span><span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">return</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">double</span><span class="token punctuation">)</span>tp<span class="token punctuation">.</span>tv_sec<span class="token operator">+</span><span class="token punctuation">(</span><span class="token keyword">double</span><span class="token punctuation">)</span>tp<span class="token punctuation">.</span>tv_usec<span class="token operator">*</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">initialData</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> ip<span class="token punctuation">,</span><span class="token keyword">int</span> size<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token class-name">time_t</span> t<span class="token punctuation">;</span>
  <span class="token function">srand</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token punctuation">)</span><span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>size<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>
  <span class="token punctuation">&#123;</span>
    ip<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token function">rand</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&amp;</span><span class="token number">0xffff</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">1000.0f</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">initialData_int</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token operator">*</span> ip<span class="token punctuation">,</span> <span class="token keyword">int</span> size<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
	<span class="token class-name">time_t</span> t<span class="token punctuation">;</span>
	<span class="token function">srand</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">unsigned</span><span class="token punctuation">)</span><span class="token function">time</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		ip<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token keyword">int</span><span class="token punctuation">(</span><span class="token function">rand</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&amp;</span><span class="token number">0xff</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">initDevice</span><span class="token punctuation">(</span><span class="token keyword">int</span> devNum<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">int</span> dev <span class="token operator">=</span> devNum<span class="token punctuation">;</span>
  cudaDeviceProp deviceProp<span class="token punctuation">;</span>
  <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaGetDeviceProperties</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>deviceProp<span class="token punctuation">,</span>dev<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Using device %d: %s\n"</span><span class="token punctuation">,</span>dev<span class="token punctuation">,</span>deviceProp<span class="token punctuation">.</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>dev<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">checkResult</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span> hostRef<span class="token punctuation">,</span><span class="token keyword">float</span> <span class="token operator">*</span> gpuRef<span class="token punctuation">,</span><span class="token keyword">const</span> <span class="token keyword">int</span> N<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">double</span> epsilon<span class="token operator">=</span><span class="token number">1.0E-8</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>N<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span>
  <span class="token punctuation">&#123;</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">abs</span><span class="token punctuation">(</span>hostRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">-</span>gpuRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">></span>epsilon<span class="token punctuation">)</span>
    <span class="token punctuation">&#123;</span>
      <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Results don\'t match!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%f(hostRef[%d] )!= %f(gpuRef[%d])\n"</span><span class="token punctuation">,</span>hostRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>i<span class="token punctuation">,</span>gpuRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token keyword">return</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Check result success!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>vector_add_gpu.cu</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;assert.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;time.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"cudastart.h"</span></span>

<span class="token keyword">const</span> <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">28</span><span class="token punctuation">;</span>

<span class="token keyword">inline</span> <span class="token class-name">cudaError_t</span> <span class="token function">checkCuda</span><span class="token punctuation">(</span><span class="token class-name">cudaError_t</span> result<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>result <span class="token operator">!=</span> cudaSuccess<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token function">fprintf</span><span class="token punctuation">(</span><span class="token constant">stderr</span><span class="token punctuation">,</span> <span class="token string">"CUDA Runtime Error: %s\n"</span><span class="token punctuation">,</span> <span class="token function">cudaGetErrorString</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">assert</span><span class="token punctuation">(</span>result <span class="token operator">==</span> cudaSuccess<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token keyword">return</span> result<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">initWith</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>a<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span>
  <span class="token punctuation">&#123;</span>
    a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">addVectorsInto</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>result<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>b<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">int</span> index <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">int</span> gridstride <span class="token operator">=</span> gridDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> index<span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i <span class="token operator">+=</span> gridstride<span class="token punctuation">)</span>
      result<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> b<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
  
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">checkElementsAre</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>array<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
  <span class="token punctuation">&#123;</span>
    <span class="token keyword">if</span><span class="token punctuation">(</span>array<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token number">2</span> <span class="token operator">*</span> i<span class="token punctuation">)</span>
    <span class="token punctuation">&#123;</span>
      <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"FAIL: array[%d] - %d does not equal %d\n"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> array<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
      <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"SUCCESS! All values added correctly.\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">cpuAdd</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>h_a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>h_b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>h_c<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> tid <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> N<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        h_c<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> h_a<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+</span> h_b<span class="token punctuation">[</span>tid<span class="token punctuation">]</span><span class="token punctuation">;</span>
        tid <span class="token operator">+=</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>


<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
  <span class="token class-name">size_t</span> size <span class="token operator">=</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  
  <span class="token keyword">int</span> <span class="token operator">*</span>cpu_a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">int</span> <span class="token operator">*</span>cpu_b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">int</span> <span class="token operator">*</span>cpu_c <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token function">initWith</span><span class="token punctuation">(</span>cpu_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">initWith</span><span class="token punctuation">(</span>cpu_b<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token keyword">double</span> start_cpu <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cpuAdd</span><span class="token punctuation">(</span>cpu_a<span class="token punctuation">,</span> cpu_b<span class="token punctuation">,</span> cpu_c<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">double</span> end_cpu <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">checkElementsAre</span><span class="token punctuation">(</span>cpu_c<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"vector add, CPU Time used: %f ms\n"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>end_cpu <span class="token operator">-</span> start_cpu<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">free</span><span class="token punctuation">(</span>cpu_a<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">free</span><span class="token punctuation">(</span>cpu_b<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">free</span><span class="token punctuation">(</span>cpu_c<span class="token punctuation">)</span><span class="token punctuation">;</span>
  
  <span class="token keyword">int</span> <span class="token operator">*</span>a<span class="token punctuation">;</span>
  <span class="token keyword">int</span> <span class="token operator">*</span>b<span class="token punctuation">;</span>
  <span class="token keyword">int</span> <span class="token operator">*</span>c<span class="token punctuation">;</span>
  <span class="token keyword">int</span> deviceId<span class="token punctuation">;</span>
  <span class="token function">cudaGetDevice</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>deviceId<span class="token punctuation">)</span><span class="token punctuation">;</span>


  <span class="token function">checkCuda</span><span class="token punctuation">(</span><span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>a<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">checkCuda</span><span class="token punctuation">(</span><span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>b<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">checkCuda</span><span class="token punctuation">(</span><span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>c<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  
  
  <span class="token function">cudaMemPrefetchAsync</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> size<span class="token punctuation">,</span> cudaCpuDeviceId<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaMemPrefetchAsync</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> size<span class="token punctuation">,</span> cudaCpuDeviceId<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">initWith</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">initWith</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaMemPrefetchAsync</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> size<span class="token punctuation">,</span> deviceId<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaMemPrefetchAsync</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> size<span class="token punctuation">,</span> deviceId<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token class-name">size_t</span> threadsPerBlock <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span>
  <span class="token class-name">size_t</span> numberOfBlock <span class="token operator">=</span> <span class="token punctuation">(</span>N <span class="token operator">+</span> threadsPerBlock <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> threadsPerBlock<span class="token punctuation">;</span>
  
  <span class="token keyword">double</span> start <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  addVectorsInto<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>numberOfBlock<span class="token punctuation">,</span> threadsPerBlock<span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">checkCuda</span><span class="token punctuation">(</span><span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">double</span> end <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">checkElementsAre</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// Calculate elapsed time</span>
  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"vector add, GPU Time used: %f ms\n"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>end <span class="token operator">-</span> start<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token function">cudaFree</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这段代码需要向GPU申请显存，用以存储数组。</p>
<p><code>cudaMallocManaged</code> 和 <code>cudaMalloc</code> 都是 CUDA 中用于分配设备内存的函数，它们之间有几个重要的区别：</p>
<ol>
<li><strong>管理方式</strong>：<ul>
<li><code>cudaMallocManaged</code> 分配的内存是统一内存 (Unified Memory)，可由 CPU 和 GPU 共享，无需显式地进行数据传输。这使得程序员可以更轻松地编写并行代码，而不必担心内存管理和数据传输。</li>
<li><code>cudaMalloc</code> 分配的内存则是显式地分配给 GPU 使用的内存，需要通过显式的数据传输函数（如 <code>cudaMemcpy</code>）来在 CPU 和 GPU 之间传输数据。</li>
</ul>
</li>
<li><strong>自动数据迁移</strong>：<ul>
<li>统一内存由 CUDA 运行时自动管理数据的迁移。当 CPU 或 GPU 尝试访问未分配到当前设备的统一内存时，CUDA 运行时会自动将数据迁移到访问的设备上。</li>
<li>对于 <code>cudaMalloc</code> 分配的内存，需要手动使用 <code>cudaMemcpy</code> 等函数进行数据传输。</li>
</ul>
</li>
<li><strong>便利性</strong>：<ul>
<li>使用 <code>cudaMallocManaged</code> 更加方便，因为无需手动管理数据的迁移和分配，程序员可以更专注于算法和逻辑的实现。</li>
<li><code>cudaMalloc</code> 则需要更多的手动管理，包括数据传输和内存释放。</li>
</ul>
</li>
</ol>
<p>优缺点：</p>
<ul>
<li><code>cudaMallocManaged</code> 的优点在于简化了内存管理和数据传输，提高了编程的便利性和代码的可读性。同时，由于统一内存的存在，可以减少内存使用上的一些烦琐问题。</li>
<li><code>cudaMalloc</code> 的优点在于更加灵活，程序员可以精确地控制内存的分配和数据传输，适用于需要更细粒度控制的情况。此外，对于某些特定的算法和应用场景，手动管理内存和数据传输可能会比统一内存更加高效。</li>
</ul>
<p>综上所述，选择使用 <code>cudaMallocManaged</code> 还是 <code>cudaMalloc</code> 取决于具体的应用场景和需求。</p>
<h3 id="Reduction"><a href="#Reduction" class="headerlink" title="Reduction"></a>Reduction</h3><p>对数组进行归约操作。</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"cudastart.h"</span></span>

__global__ <span class="token keyword">void</span> <span class="token function">reduce_test</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>g_idata<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>sum<span class="token punctuation">,</span> <span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
	<span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>idx <span class="token operator">&lt;</span> n<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
		<span class="token function">atomicAdd</span><span class="token punctuation">(</span>sum<span class="token punctuation">,</span> g_idata<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token comment">// sum[0] += g_idata[idx];</span>
	<span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">reduceNeighbored</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span> g_idata<span class="token punctuation">,</span><span class="token keyword">int</span> <span class="token operator">*</span> g_odata<span class="token punctuation">,</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> 
<span class="token punctuation">&#123;</span>
	<span class="token comment">//set thread ID</span>
	<span class="token keyword">unsigned</span> <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token comment">//boundary check</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">>=</span> n<span class="token punctuation">)</span> <span class="token keyword">return</span><span class="token punctuation">;</span>
	<span class="token comment">//convert global data pointer to the </span>
	<span class="token keyword">int</span> <span class="token operator">*</span>idata <span class="token operator">=</span> g_idata <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token comment">//in-place reduction in global memory</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> stride <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> stride <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> stride <span class="token operator">*=</span> <span class="token number">2</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>tid <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> stride<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
		<span class="token punctuation">&#123;</span>
			idata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+=</span> idata<span class="token punctuation">[</span>tid <span class="token operator">+</span> stride<span class="token punctuation">]</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		<span class="token comment">//synchronize within block</span>
		<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
	<span class="token comment">//write result for this block to global mem</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
		g_odata<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> idata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>

<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">reduceNeighboredLess</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span> g_idata<span class="token punctuation">,</span><span class="token keyword">int</span> <span class="token operator">*</span>g_odata<span class="token punctuation">,</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
	<span class="token keyword">unsigned</span> <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">unsigned</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token comment">// convert global data pointer to the local point of this block</span>
	<span class="token keyword">int</span> <span class="token operator">*</span>idata <span class="token operator">=</span> g_idata <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>idx <span class="token operator">></span> n<span class="token punctuation">)</span>
		<span class="token keyword">return</span><span class="token punctuation">;</span>
	<span class="token comment">//in-place reduction in global memory</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> stride <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> stride <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> stride <span class="token operator">*=</span> <span class="token number">2</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		<span class="token comment">//convert tid into local array index</span>
		<span class="token keyword">int</span> index <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> stride <span class="token operator">*</span>tid<span class="token punctuation">;</span>
		<span class="token keyword">if</span> <span class="token punctuation">(</span>index <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">)</span>
		<span class="token punctuation">&#123;</span>
			idata<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">+=</span> idata<span class="token punctuation">[</span>index <span class="token operator">+</span> stride<span class="token punctuation">]</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
	<span class="token comment">//write result for this block to global men</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
		g_odata<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> idata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">reduceInterleaved</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span> g_idata<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>g_odata<span class="token punctuation">,</span> <span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
	<span class="token keyword">unsigned</span> <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">unsigned</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token comment">// convert global data pointer to the local point of this block</span>
	<span class="token keyword">int</span> <span class="token operator">*</span>idata <span class="token operator">=</span> g_idata <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>idx <span class="token operator">>=</span> n<span class="token punctuation">)</span>
		<span class="token keyword">return</span><span class="token punctuation">;</span>
	<span class="token comment">//in-place reduction in global memory</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> stride <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">;</span> stride <span class="token operator">></span><span class="token number">0</span><span class="token punctuation">;</span> stride <span class="token operator">>>=</span><span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		
		<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span>stride<span class="token punctuation">)</span>
		<span class="token punctuation">&#123;</span>
			idata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+=</span> idata<span class="token punctuation">[</span>tid <span class="token operator">+</span> stride<span class="token punctuation">]</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
	<span class="token comment">//write result for this block to global men</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
		g_odata<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> idata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">reduceInterleaved_share</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span> g_idata<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>g_odata<span class="token punctuation">,</span> <span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>	
	__shared__ <span class="token keyword">int</span> sh_arr<span class="token punctuation">[</span><span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
	<span class="token keyword">unsigned</span> <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">unsigned</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>idx <span class="token operator">>=</span> n<span class="token punctuation">)</span>
		<span class="token keyword">return</span><span class="token punctuation">;</span>
	sh_arr<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> g_idata<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">;</span>
	<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token comment">//in-place reduction in global memory</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> stride <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">;</span> stride <span class="token operator">></span><span class="token number">0</span><span class="token punctuation">;</span> stride <span class="token operator">>>=</span><span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		
		<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span>stride<span class="token punctuation">)</span>
		<span class="token punctuation">&#123;</span>
			sh_arr<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+=</span> sh_arr<span class="token punctuation">[</span>tid <span class="token operator">+</span> stride<span class="token punctuation">]</span><span class="token punctuation">;</span>
		<span class="token punctuation">&#125;</span>
		<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
	<span class="token comment">//write result for this block to global men</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
		g_odata<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> sh_arr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span><span class="token keyword">char</span><span class="token operator">*</span><span class="token operator">*</span> argv<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
	<span class="token function">initDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	<span class="token comment">//initialization</span>

	<span class="token keyword">int</span> size <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">24</span><span class="token punctuation">;</span>
	<span class="token comment">// printf("	with array size %d  ", size);</span>

	<span class="token comment">//execution configuration</span>
	<span class="token keyword">int</span> blocksize <span class="token operator">=</span> <span class="token number">1024</span><span class="token punctuation">;</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>argc <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		blocksize <span class="token operator">=</span> <span class="token function">atoi</span><span class="token punctuation">(</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment">//从命令行输入设置block大小</span>
	<span class="token punctuation">&#125;</span>
	dim3 <span class="token function">block</span><span class="token punctuation">(</span>blocksize<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	dim3 <span class="token function">grid</span><span class="token punctuation">(</span><span class="token punctuation">(</span>size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> block<span class="token punctuation">.</span>x <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"grid %d block %d \n"</span><span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">,</span> block<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">//allocate host memory</span>
	<span class="token class-name">size_t</span> bytes <span class="token operator">=</span> size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">int</span> <span class="token operator">*</span>idata_host <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">int</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">int</span> <span class="token operator">*</span>odata_host <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">int</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>grid<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">int</span> <span class="token operator">*</span> tmp <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">int</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">//initialize the array</span>
	<span class="token function">initialData_int</span><span class="token punctuation">(</span>idata_host<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token keyword">if</span> <span class="token punctuation">(</span>size <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Array: ["</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span>
			<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%d, "</span><span class="token punctuation">,</span> idata_host<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
		<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"]\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>

	<span class="token function">memcpy</span><span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> idata_host<span class="token punctuation">,</span> bytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">double</span> timeStart<span class="token punctuation">,</span> timeElaps<span class="token punctuation">;</span>
	<span class="token keyword">int</span> gpu_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

	<span class="token comment">// device memory</span>
	<span class="token keyword">int</span> <span class="token operator">*</span> idata_dev <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>
	<span class="token keyword">int</span> <span class="token operator">*</span> odata_dev <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>idata_dev<span class="token punctuation">,</span> bytes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>odata_dev<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">//cpu reduction 对照组</span>
	<span class="token keyword">int</span> cpu_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
	timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
		cpu_sum <span class="token operator">+=</span> tmp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
	timeElaps <span class="token operator">=</span> <span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"cpu sum:%d \n"</span><span class="token punctuation">,</span> cpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"cpu reduction elapsed %lf ms cpu_sum: %d\n"</span><span class="token punctuation">,</span> timeElaps<span class="token punctuation">,</span> cpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">//kernel 0 reduce</span>
	<span class="token keyword">int</span> <span class="token operator">*</span>reduce_sum<span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>reduce_sum<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemset</span><span class="token punctuation">(</span>reduce_sum<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> idata_host<span class="token punctuation">,</span> bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	reduce_test <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block <span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> reduce_sum<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>gpu_sum<span class="token punctuation">,</span> reduce_sum<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu sum:%d \n"</span><span class="token punctuation">,</span> gpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu atomicAdd elapsed %lf ms     &lt;&lt;&lt;grid %d block %d>>>\n"</span><span class="token punctuation">,</span>
		timeElaps<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">,</span> block<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaFree</span><span class="token punctuation">(</span>reduce_sum<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	
	<span class="token comment">//kernel 1 reduceNeighbored</span>

	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> idata_host<span class="token punctuation">,</span> bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	reduceNeighbored <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block <span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>odata_host<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
	gpu_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
		gpu_sum <span class="token operator">+=</span> odata_host<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>	
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu sum:%d \n"</span><span class="token punctuation">,</span> gpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu reduceNeighbored elapsed %lf ms     &lt;&lt;&lt;grid %d block %d>>>\n"</span><span class="token punctuation">,</span>
		timeElaps<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">,</span> block<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">//kernel 2 reduceNeighboredless</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> idata_host<span class="token punctuation">,</span> bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	reduceNeighboredLess <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block <span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>odata_host<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
	gpu_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
		gpu_sum <span class="token operator">+=</span> odata_host<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>	
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu sum:%d \n"</span><span class="token punctuation">,</span> gpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu reduceNeighboredless elapsed %lf ms     &lt;&lt;&lt;grid %d block %d>>>\n"</span><span class="token punctuation">,</span>
		timeElaps<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">,</span> block<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">//kernel 3 reduceInterleaved</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> idata_host<span class="token punctuation">,</span> bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	reduceInterleaved <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block <span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>odata_host<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
	gpu_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
		gpu_sum <span class="token operator">+=</span> odata_host<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>	
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu sum:%d \n"</span><span class="token punctuation">,</span> gpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu reduceInterleaved elapsed %lf ms     &lt;&lt;&lt;grid %d block %d>>>\n"</span><span class="token punctuation">,</span>
		timeElaps<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">,</span> block<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
	<span class="token comment">//kernel 4 reduceInterleaved shared memory</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> idata_host<span class="token punctuation">,</span> bytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	reduceInterleaved_share <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block <span class="token operator">>></span><span class="token operator">></span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>odata_host<span class="token punctuation">,</span> odata_dev<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
	gpu_sum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
	<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span>
		gpu_sum <span class="token operator">+=</span> odata_host<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>	
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu sum:%d \n"</span><span class="token punctuation">,</span> gpu_sum<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu reduceInterleaved elapsed %lf ms     &lt;&lt;&lt;grid %d block %d>>>\n"</span><span class="token punctuation">,</span>
		timeElaps<span class="token punctuation">,</span> grid<span class="token punctuation">.</span>x<span class="token punctuation">,</span> block<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>


	<span class="token comment">// free host memory</span>

	<span class="token function">free</span><span class="token punctuation">(</span>idata_host<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">free</span><span class="token punctuation">(</span>odata_host<span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaFree</span><span class="token punctuation">(</span>idata_dev<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token function">CHECK</span><span class="token punctuation">(</span><span class="token function">cudaFree</span><span class="token punctuation">(</span>odata_dev<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">//reset device</span>
	<span class="token function">cudaDeviceReset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

	<span class="token comment">//check the results</span>
	<span class="token keyword">if</span> <span class="token punctuation">(</span>gpu_sum <span class="token operator">==</span> cpu_sum<span class="token punctuation">)</span>
	<span class="token punctuation">&#123;</span>
		<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Test success!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
	<span class="token punctuation">&#125;</span>
	<span class="token keyword">return</span> EXIT_SUCCESS<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Matrix-multiplication"><a href="#Matrix-multiplication" class="headerlink" title="Matrix multiplication"></a>Matrix multiplication</h3><p>计算二维矩阵乘法。</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"cudastart.h"</span></span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">N</span>  <span class="token expression"><span class="token number">1024</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">tile_size</span> <span class="token expression"><span class="token number">16</span></span></span>

<span class="token keyword">void</span> <span class="token function">matrixMulCPU</span><span class="token punctuation">(</span> <span class="token keyword">int</span> <span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> c <span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> row <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> row <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>row <span class="token punctuation">)</span>
        <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> col <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> col <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>col <span class="token punctuation">)</span>
        <span class="token punctuation">&#123;</span>
            val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span> <span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>k <span class="token punctuation">)</span>
                val <span class="token operator">+=</span> a<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>k <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span>
            c<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span>
        <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">matrixMulGPU</span><span class="token punctuation">(</span> <span class="token keyword">int</span> <span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> c <span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> row <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> col <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    <span class="token keyword">int</span> val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
        val <span class="token operator">+=</span> a<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>i <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
    c<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>

__global__ <span class="token keyword">void</span> <span class="token function">matrixGPU_tile</span><span class="token punctuation">(</span> <span class="token keyword">int</span> <span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> c <span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
    __shared__ <span class="token keyword">int</span> tile_a<span class="token punctuation">[</span>tile_size<span class="token punctuation">]</span><span class="token punctuation">[</span>tile_size<span class="token punctuation">]</span><span class="token punctuation">;</span>
    __shared__ <span class="token keyword">int</span> tile_b<span class="token punctuation">[</span>tile_size<span class="token punctuation">]</span><span class="token punctuation">[</span>tile_size<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> row <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
    <span class="token keyword">int</span> col <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N <span class="token operator">/</span> tile_size<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
        tile_a<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> i <span class="token operator">*</span> tile_size <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span><span class="token punctuation">;</span>
        tile_b<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> b<span class="token punctuation">[</span><span class="token punctuation">(</span>i <span class="token operator">*</span> tile_size <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> tile_size<span class="token punctuation">;</span> <span class="token operator">++</span>j<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
            c<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">+=</span> tile_a<span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">*</span> tile_b<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">[</span>threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token punctuation">&#125;</span>
        <span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">void</span> <span class="token function">check</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>c_cpu<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>c_gpu<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
    bool error <span class="token operator">=</span> false<span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> row <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> row <span class="token operator">&lt;</span> N <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span>error<span class="token punctuation">;</span> <span class="token operator">++</span>row <span class="token punctuation">)</span>
        <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> col <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> col <span class="token operator">&lt;</span> N <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span>error<span class="token punctuation">;</span> <span class="token operator">++</span>col <span class="token punctuation">)</span><span class="token punctuation">&#123;</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>c_cpu<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">!=</span> c_gpu<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">)</span>
                <span class="token punctuation">&#123;</span>
                    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"FOUND ERROR at c[%d][%d]\n"</span><span class="token punctuation">,</span> row<span class="token punctuation">,</span> col<span class="token punctuation">)</span><span class="token punctuation">;</span>
                    error <span class="token operator">=</span> true<span class="token punctuation">;</span>
                    <span class="token keyword">break</span><span class="token punctuation">;</span>
                <span class="token punctuation">&#125;</span>
    <span class="token punctuation">&#125;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>error<span class="token punctuation">)</span>
        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Success!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>


<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
    <span class="token keyword">int</span> <span class="token operator">*</span>a<span class="token punctuation">,</span> <span class="token operator">*</span>b<span class="token punctuation">,</span> <span class="token operator">*</span>c_cpu<span class="token punctuation">,</span> <span class="token operator">*</span>c_gpu<span class="token punctuation">,</span> <span class="token operator">*</span>c_gpu_opt<span class="token punctuation">;</span> <span class="token comment">// Allocate a solution matrix for both the CPU and the GPU operations</span>
    <span class="token keyword">int</span> size <span class="token operator">=</span> N <span class="token operator">*</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span> <span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// Number of bytes of an N x N matrix</span>
    <span class="token keyword">double</span> timeStart<span class="token punctuation">,</span> timeElaps<span class="token punctuation">;</span>
    
    <span class="token comment">// Allocate memory</span>
    <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>a<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>b<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>c_cpu<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>c_gpu<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>c_gpu_opt<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token function">initialData_int</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> N <span class="token operator">*</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">initialData_int</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> N <span class="token operator">*</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">memset</span><span class="token punctuation">(</span>c_cpu<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> N <span class="token operator">*</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">memset</span><span class="token punctuation">(</span>c_gpu<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> N <span class="token operator">*</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">memset</span><span class="token punctuation">(</span>c_gpu_opt<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> N <span class="token operator">*</span> N<span class="token punctuation">)</span><span class="token punctuation">;</span>

    timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">matrixMulCPU</span><span class="token punctuation">(</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c_cpu <span class="token punctuation">)</span><span class="token punctuation">;</span>
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"cpu matrix mul time: %f ms\n"</span><span class="token punctuation">,</span> timeElaps<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// test kernel 1: matrixMulGPU</span>
    dim3 <span class="token function">threads_per_block</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    dim3 <span class="token function">number_of_blocks</span><span class="token punctuation">(</span>N <span class="token operator">/</span> threads_per_block<span class="token punctuation">.</span>x<span class="token punctuation">,</span> N <span class="token operator">/</span> threads_per_block<span class="token punctuation">.</span>y<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    matrixMulGPU <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> number_of_blocks<span class="token punctuation">,</span> threads_per_block <span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">(</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c_gpu <span class="token punctuation">)</span><span class="token punctuation">;</span>
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu matrix mul time: %f ms\n"</span><span class="token punctuation">,</span> timeElaps<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">check</span><span class="token punctuation">(</span>c_cpu<span class="token punctuation">,</span> c_gpu<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">// test kernel 2: matrixMulGPU optimize</span>
    timeStart <span class="token operator">=</span> <span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    matrixGPU_tile <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> number_of_blocks<span class="token punctuation">,</span> threads_per_block <span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">(</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c_gpu_opt <span class="token punctuation">)</span><span class="token punctuation">;</span>
    timeElaps <span class="token operator">=</span> <span class="token number">1000</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token function">cpuSecond</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> timeStart<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"gpu matrix mul time: %f ms\n"</span><span class="token punctuation">,</span> timeElaps<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">check</span><span class="token punctuation">(</span>c_cpu<span class="token punctuation">,</span> c_gpu_opt<span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token comment">// Free all our allocated memory</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span> c_cpu <span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span> c_gpu <span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span> c_gpu_opt <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="CUDNN"><a href="#CUDNN" class="headerlink" title="CUDNN"></a>CUDNN</h3><p>使用cuDNN实现sigmoid函数</p>
<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cuda_runtime.h></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cudnn.h></span></span>

<span class="token comment">/**
 * Minimal example to apply sigmoid activation on a tensor
 * using cuDNN.
 * https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-graph-library.html#
 * https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-ops-library.html#cudnnactivationforward
 **/</span>
<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span><span class="token operator">*</span><span class="token operator">*</span> argv<span class="token punctuation">)</span>
<span class="token punctuation">&#123;</span>
    <span class="token comment">// get gpu info</span>
    <span class="token keyword">int</span> numGPUs<span class="token punctuation">;</span>
    <span class="token function">cudaGetDeviceCount</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>numGPUs<span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Found "</span> <span class="token operator">&lt;&lt;</span> numGPUs <span class="token operator">&lt;&lt;</span> <span class="token string">" GPUs."</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// use GPU0</span>
    <span class="token keyword">int</span> device<span class="token punctuation">;</span>
    <span class="token keyword">struct</span> <span class="token class-name">cudaDeviceProp</span> devProp<span class="token punctuation">;</span>
    <span class="token function">cudaGetDevice</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>device<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaGetDeviceProperties</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>devProp<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Compute capability:"</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>major <span class="token operator">&lt;&lt;</span> <span class="token string">"."</span> <span class="token operator">&lt;&lt;</span> devProp<span class="token punctuation">.</span>minor <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>

    <span class="token class-name">cudnnHandle_t</span> handle_<span class="token punctuation">;</span>
    <span class="token function">cudnnCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>handle_<span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Created cuDNN handle"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>

    <span class="token comment">// create the tensor descriptor</span>
    <span class="token class-name">cudnnDataType_t</span> dtype <span class="token operator">=</span> CUDNN_DATA_FLOAT<span class="token punctuation">;</span>
    <span class="token class-name">cudnnTensorFormat_t</span> format <span class="token operator">=</span> CUDNN_TENSOR_NCHW<span class="token punctuation">;</span>
    <span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> c <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> h <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> w <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">;</span>
    <span class="token keyword">int</span> NUM_ELEMENTS <span class="token operator">=</span> n<span class="token operator">*</span>c<span class="token operator">*</span>h<span class="token operator">*</span>w<span class="token punctuation">;</span>
    <span class="token class-name">cudnnTensorDescriptor_t</span> x_desc<span class="token punctuation">;</span>
    <span class="token function">cudnnCreateTensorDescriptor</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>x_desc<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudnnSetTensor4dDescriptor</span><span class="token punctuation">(</span>x_desc<span class="token punctuation">,</span> format<span class="token punctuation">,</span> dtype<span class="token punctuation">,</span> n<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">// create the tensor</span>
    <span class="token keyword">float</span> <span class="token operator">*</span>x<span class="token punctuation">;</span>

    <span class="token comment">// 创建 Unified Memory，这样cpu和memory都可以使用</span>
    <span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>x<span class="token punctuation">,</span> NUM_ELEMENTS <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>NUM_ELEMENTS<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> i <span class="token operator">*</span> <span class="token number">1.00f</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Original array: "</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>NUM_ELEMENTS<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span> std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" "</span><span class="token punctuation">;</span>

    <span class="token comment">// create activation function descriptor</span>
    <span class="token keyword">float</span> alpha<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>
    <span class="token keyword">float</span> beta<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">0.0</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>
    <span class="token class-name">cudnnActivationDescriptor_t</span> sigmoid_activation<span class="token punctuation">;</span>
    <span class="token class-name">cudnnActivationMode_t</span> mode <span class="token operator">=</span> CUDNN_ACTIVATION_SIGMOID<span class="token punctuation">;</span>
    <span class="token class-name">cudnnNanPropagation_t</span> prop <span class="token operator">=</span> CUDNN_NOT_PROPAGATE_NAN<span class="token punctuation">;</span>
    <span class="token function">cudnnCreateActivationDescriptor</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>sigmoid_activation<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudnnSetActivationDescriptor</span><span class="token punctuation">(</span>sigmoid_activation<span class="token punctuation">,</span> mode<span class="token punctuation">,</span> prop<span class="token punctuation">,</span> <span class="token number">0.0f</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token function">cudnnActivationForward</span><span class="token punctuation">(</span>
        handle_<span class="token punctuation">,</span>
        sigmoid_activation<span class="token punctuation">,</span>
        alpha<span class="token punctuation">,</span>
        x_desc<span class="token punctuation">,</span>
        x<span class="token punctuation">,</span>
        beta<span class="token punctuation">,</span>
        x_desc<span class="token punctuation">,</span>
        x
    <span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token function">cudnnDestroy</span><span class="token punctuation">(</span>handle_<span class="token punctuation">)</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl <span class="token operator">&lt;&lt;</span> <span class="token string">"Destroyed cuDNN handle."</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"New array: "</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span>i<span class="token operator">&lt;</span>NUM_ELEMENTS<span class="token punctuation">;</span>i<span class="token operator">++</span><span class="token punctuation">)</span> std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" "</span><span class="token punctuation">;</span>
    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span>
    <span class="token function">cudaFree</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>cuDNN文档： <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-graph-library.html">https://docs.nvidia.com/deeplearning/cudnn/latest/api/cudnn-graph-library.html</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://qiankunli.github.io/2021/08/18/gpu.html">GPU入门 | 李乾坤的博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/98416987">CUDA编程入门（五）更高效的并行归约算法 - ZihaoZhao的文章 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/123170285">理解CUDA中的thread,block,grid和warp - 三七和酒的文章 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/468048296">CUDNN（1） - 初步体验 - 颜挺帅的文章 - 知乎</a></p>
<p>《分布式与并行计算技术》 刘莹</p>
<p>《计算机体系结构》 张科  刘珂  高婉铃</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Knowledge</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://knowledge-llz.github.io/2024/04/25/cuda/">http://knowledge-llz.github.io/2024/04/25/cuda/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Knowledge</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/cuda%E7%BC%96%E7%A8%8B/">
                                    <span class="chip bg-color">cuda编程</span>
                                </a>
                            
                                <a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">
                                    <span class="chip bg-color">并行计算</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2024/04/25/cuda/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/22.jpg" class="responsive-img" alt="从零开始的CUDA编程">
                        
                        <span class="card-title">从零开始的CUDA编程</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-04-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                    高性能计算
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/cuda%E7%BC%96%E7%A8%8B/">
                        <span class="chip bg-color">cuda编程</span>
                    </a>
                    
                    <a href="/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/">
                        <span class="chip bg-color">并行计算</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/01/12/computer-architecture-interconnect/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="计算机体系结构-互联网络">
                        
                        <span class="card-title">计算机体系结构-互联网络</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-01-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/" class="post-category">
                                    计算机体系结构
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/interconnection/">
                        <span class="chip bg-color">interconnection</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2022-2024</span>
            
            <a href="/about" target="_blank">Knowledge</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">117.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2022";
                        var startMonth = "3";
                        var startDate = "28";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/knowledge-llz" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:925538513@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=925538513" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 925538513" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
